{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "798ac605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to load files\n",
    "file_path = \"./final-ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2b5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer\n",
    "from sklearn.metrics import auc, f1_score, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e47977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos de test\n",
    "x_test, y_test = pickle.load(open(file_path+'/final_test.pkl', 'rb'))\n",
    "\n",
    "# Diccionarios\n",
    "saved_dict = pickle.load(open(file_path+'/saved_dict.pkl', 'rb'))\n",
    "mode_dict = pickle.load(open(file_path+'/mode_dict.pkl', 'rb'))\n",
    "\n",
    "# Standard scaler\n",
    "scaler = pickle.load(open(file_path+'/scaler.pkl', 'rb'))\n",
    "\n",
    "# Encoders one-hot\n",
    "ohe_proto = pickle.load(open(file_path+'/ohe_proto.pkl', 'rb'))\n",
    "ohe_service = pickle.load(open(file_path+'/ohe_service.pkl', 'rb'))\n",
    "ohe_state = pickle.load(open(file_path+'/ohe_state.pkl', 'rb'))\n",
    "\n",
    "# El mejor modelo, es el Randomforest Classifier \n",
    "best_model = pickle.load(open(file_path+'/rf_best_clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b306737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpieza de datos\n",
    "\n",
    "def clean_data(data):\n",
    "\n",
    "    #Elimina los valores nulos y erróneos \n",
    "    \n",
    "    numerical_col = data.select_dtypes(include=np.number).columns \n",
    "    categorical_col = data.select_dtypes(exclude=np.number).columns\n",
    "    \n",
    "    # Cleaning the data\n",
    "    for col in data.columns:\n",
    "        val = mode_dict[col]  # Sustituye los valores nulos por el valor modal\n",
    "        data[col] = data[col].fillna(value=val)\n",
    "        data[col] = data[col].replace(' ', value=val)\n",
    "        data[col] = data[col].apply(lambda x:\"None\" if x==\"-\" else x)\n",
    "\n",
    "        # Si son binarias y el valor es >1 se sustituye por el valor modal\n",
    "        if col in saved_dict['binary_col']:\n",
    "            data[col] = np.where(data[col]>1, val, data[col])\n",
    "\n",
    "    # Convertir columnas de tipo erróneo\n",
    "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
    "    for bad_col in bad_dtypes:\n",
    "        data[col] = data[col].astype(float)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#Aplicar log1p\n",
    "\n",
    "def apply_log1p(data):\n",
    "    \n",
    "    #Ejecuta el log1p, crea una nueva columna y elimina la original\n",
    "    \n",
    "    for col in saved_dict['log1p_col']:\n",
    "        new_col = col + '_log1p'\n",
    "        data[new_col] = data[col].apply(np.log1p)\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "#Estandarizar los valores\n",
    "\n",
    "def standardize(data):\n",
    "    \n",
    "    #Estandariza las columnas numéricas\n",
    "    \n",
    "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
    "    return data\n",
    "\n",
    "\n",
    "#Encoders one-hot para las columnas categóricas\n",
    "\n",
    "def ohencoding(data):\n",
    "    '''\n",
    "    Onehot encoding the categoricla columns.\n",
    "    Add the ohe columns with the data and removes categorical columns.\n",
    "    Using Onehotencoder objects trained on train data.\n",
    "    '''\n",
    "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
    "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
    "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
    "    \n",
    "    #Incluir las columnas con el encoder y eliminar las originales\n",
    "    \n",
    "    data = pd.concat([data,\n",
    "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
    "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
    "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
    "                      axis=1)\n",
    "    \n",
    "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0eb2d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para realizar el preprocesamiento y las predicciones a partir de los datos\n",
    "\n",
    "def final_fun_1(X):\n",
    "   \n",
    "    if isinstance(X, pd.core.series.Series):\n",
    "        \n",
    "        # Para una entrada puntual\n",
    "        data = pd.DataFrame(X.values.reshape(1, -1)).copy()\n",
    "    else:\n",
    "        data = X.copy()\n",
    " \n",
    "\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.columns = saved_dict['columns']\n",
    "\n",
    "\n",
    "    dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
    "    data.drop(columns=dropable_col, inplace=True)\n",
    "\n",
    "    data = clean_data(data)\n",
    "    data = apply_log1p(data)\n",
    "    data = standardize(data)\n",
    "    data = ohencoding(data)\n",
    "\n",
    "    \n",
    "    # Realizar predicción con el modelo\n",
    "    \n",
    "    predictions = best_model.predict(data)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc12e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d0f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para una entrada puntual\n",
    "\n",
    "y_pred = final_fun_1(x_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34898c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6]), 'normal')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc58c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para un conjunto de entradas\n",
    "\n",
    "y_pred = final_fun_1(x_test.iloc[90:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce5ac358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6 6 6 5 6 6]\n",
      "['normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'normal' 'generic'\n",
      " 'normal' 'normal']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test.iloc[90:100].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee12add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para todo el conjunto de datos\n",
    "y_pred = final_fun_1(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "939f9e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 ... 6 5 6]\n",
      "['normal' 'normal' 'normal' ... 'normal' 'generic' 'normal']\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a049e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d30d588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 ... 6 5 6]\n",
      "[6 6 6 ... 6 5 6]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0740c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de la puntuación f1 \n",
    "f1 = f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9c59f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015, 48)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81504685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para la realización de predición, evaluación y visualización de los resultados del modelo\n",
    "def final_fun_2(X, Y):\n",
    "    \n",
    "    y_true = Y.copy() #Valores esperados\n",
    "    y_pred = final_fun_1(X) #Predicciones\n",
    "\n",
    "    # auc curve\n",
    "    \n",
    "    # Confusion, precison and recall matrix\n",
    "    C = confusion_matrix(y_true, y_pred)\n",
    "    P = (C/C.sum(axis=0))\n",
    "    R =(((C.T)/(C.sum(axis=1))).T)\n",
    "\n",
    "    test_classification_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    y_auc = test_classification_report['weighted avg']['precision']\n",
    "    y_f1 = test_classification_report['weighted avg']['f1-score']\n",
    "    \n",
    "    # Scores of test dataset\n",
    "    y_far = (C.sum(axis=1) - np.diag(C)).sum() / C.sum()\n",
    "    # False alarm rate\n",
    "\n",
    "    # Visualización del resultado\n",
    "    \n",
    "    x = PrettyTable()\n",
    "    x.field_names = ['AUC', 'F1-score', 'Tasa de falsa alarma']\n",
    "    x.add_row([y_auc, y_f1, y_far])\n",
    "    print(x)\n",
    "    \n",
    "    return y_auc, y_f1, y_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33406ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|        AUC         |      F1-score      | Tasa de falsa alarma |\n",
      "+--------------------+--------------------+----------------------+\n",
      "| 0.9799501542978383 | 0.9792558247116256 | 0.01900093830173947  |\n",
      "+--------------------+--------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# Resultados con todo el conjunto de datos\n",
    "\n",
    "auc, f1, far = final_fun_2(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
